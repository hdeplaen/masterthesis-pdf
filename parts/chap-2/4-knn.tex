\section{Nearest neighbour}
+ on validating the models (cross-validation not needed as we have plenty of data-points -- more than needed ---, use of a classical independent test set)

A first classification algorithm that often used in intrusion detection systems is the $k$-nearest neighbour ($k$-NN). This is a simple and effective method based on the distance of the elements in the feature space. Intuitively, when we want to characterize new data elements, we compare it to existent data elements and try to assign to it the characteristics --- in this case, the intrusion target --- of the similar ones. In other words, the test set elements are classified as similar training elements. Though, how do we count to elements to be similar, how do we measure similarity? In the $k$-NN algorithm, the elements are considered as data-points in the feature space and the similarity is measured as the euclidean distance between them. The most common target of the similar elements is then assigned to the test element.

More concretely, let's first consider a binary classifier of observations and targets $\left( x_1, y_1 \right), \ldots , \left( x_n, y_n \right)$ where observations $x_i \in \mathbb{R}^d$ and targets $y_i \in \left\{0,1\right\}$. To classify a new observation $\left( x_i, y_i \right)$, one just have to compare it to the $k$ other nearest observations and assign to the test observation the dominant target among its $k$ neighbours. In the classical $k$-NN algorithm, the neighbours are defined by the Euclidean distance. However, because of the monotony of the square function, one may use the square of the distances instead, avoiding to compute an extra square root, which is very useful in frameworks where operations are expensive like MPCs:
\begin{equation}
    \mathrm{d}^2\left( x_i, x_j \right) = \norm{x_i-x_j}^2 = \sum_{k=1}^d \left( x_{ik}, x_{jk} \right)^2
\end{equation}

Unlike binary classification algorithms like SVMs, $k$-NN algorithms are suited for multi-class problems without requiring any construction. However, they have also been used for binary classification in a construction \cite{Aburomman2016ASystem}. 

\subsection{The $k$-NN algorithm}
Let's focus now on the multi-class $k$-NN algorithm without any data reduction or optimization. The sole pre-process consists here of input normalization. The results are given at table~\ref{tab:knn-1}. A very interesting property is that 

\begin{table}[ht!]
    \centering
    \begin{tabularx}{\textwidth}{lcccccc}
    \hlineI
    Model & Normal & Probe & Dos & U2R & R2L & Total \\ \hlineI
    $k=1$ and $n=500$ & & & & & &\\
    Accuracy [\%] & 76.83 & 93.51 & 92.91 & 92.02 & 80.36 & \\
    TP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    TN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\ \hline
    
    $k=7$ and $n=500$ & & & & & &\\
    Accuracy [\%] & 76.03 & 93.35 & 91.14 & 91.38 & 82.28 & \\
    TP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    TN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\ \hline
    
    $k=1$ and $n=15000$ & & & & & &\\
    Accuracy [\%] & 75.79 & 94.56 & 91.66 & 98.89 & 81.14 & \\
    TP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    TN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\ \hline
    
    $k=7$ and $n=15000$ & & & & & &\\
    Accuracy [\%] & 74.52 & 94.46 & 90.28 & 99.56 & 82.57 & \\
    TP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    TN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FP [\%] & 1 & 1 & 1 & 1 & 1 & 1\\
    FN [\%] & 1 & 1 & 1 & 1 & 1 & 1\\ \hlineI
    \end{tabularx}
    \caption{Detailed results of the $k$-NN classification algorithm for two different values of the number of neighbours $k$ and for a big and a small training data-set. The accuracy, true positive rate (TP), true negative rate (TN), false positive rate (FP) and false negative rate (FN) are given for each target class.}
    \label{tab:knn-1}
\end{table}

\subsection{Condensed Nearest Neighbours Data Reduction}
The problem of the nearest neighbours algorithm is its time to compute for large database. First, the computation of the distances increases linearly with the number of training points for each test points, thus quadratically if the number the test set has the same size as the training set. Secondly, the finding of the nearest neighbours takes requires O(kn) datapoints ... As we will see, one of the main trade-offs of MPC is the drastic increase in execution time. Therefore, the reduction of data-points becomes a necessity for the use of MPC-based $k$-NN.

The \emph{condensed nearest neighbours data reduction} (CNN) algorithm aims at finding the sole relevant data-points in the whole training set. The unclassified data-points then only need to be compared with the prototype points and not the whole database anymore. It is based on the idea that misclassified data-points lie close to the decision boundary, and one can just keep the data-points near the decision boundary and discard the others. The goal of the algorithm is to classify all training data-points into three categories:
\begin{itemize}
    \item \textbf{Outliers:} points which would not be recognized as the correct type if added to the database later. In other words, these data-points are the ones making the model perform worse with them than without. They increase the complexity of the decision boundary and increase significantly the number of kepts data-points and thus the relevance of the algorithm as it tries to keep a minimum of points defining the decision boundary. These points are to be discarded.
    \item \textbf{Prototypes:} the minimum set of points required in the training set for all the other non-outlier points to be correctly recognized. These data-points thus contain almost all of the relevant data. These points define the decision boundary and are to be kept.
    \item \textbf{Absorbed points:} points which are not outliers, and would be correctly recognized based on the sole prototype points. This could also be characterized as the redundant data. These points are far from the decision boundary and are to be discarded.
\end{itemize}

The outliers are first found by testing all existing points against the rest of the database with the chosen number $k$ of neighbours. If the point isn't recognized as the correct class, it is considered as an outlier, otherwise not. After defining all the outliers, one can proceed into the classification the restant data-points as prototypes or absorbed points. This is done in the next manner. Each point is once again tested against the dataset without the outliers, but with one sole neighbour, i.e. $k=1$. If it is well classified, it is considered as an absorbed point and can be left out of the final dataset. Otherwise, it is considered as a prototype ans is considered relevant to the classification or not redundant. These data-points are the sole ones making it to the final dataset. All points are tested until no prototype anymore makes it into the final dataset. The CNN data-reduction is given at algorithm~\ref{alg:cnn}.

In the condensed nearest neighbours algorithm, all new data-points are tested against the reduced database. Another big advantage of this algorithm is that the algorithm now always has to be used with $k=1$ because of the way we defined our prototypes and the final algorithm is thus faster. However, the classification with CNN will most of the time lead to a slightly different classification than with the classical $k$-NN against the whole dataset. Let's now investigate the classification trade-off in the case of our intrusion detection system database. 

\begin{center}
\begin{algorithm}[H]
 \KwData{Set $\mathcal{S}$ of $n$ feature points $\left\{x_i\right\}_{i=1 \ldots n}$ and corresponding targets $\left\{y_i\right\}_{i=1 \ldots n}$, number of neighbours $k$ for outlier detection}
 \KwResult{Reduced set $\mathcal{R}$ of $m<n$ feature points $\left\{x_i\right\}_{i=1 \ldots m}$ and corresponding targets $\left\{y_i\right\}_{i=1 \ldots m}$}
\DontPrintSemicolon
\SetKwFunction{FkNN}{kNN}

\ForEach{$\left(x_i, y_i\right) \in \mathcal{S}$}{
$t \leftarrow $\FkNN{$k$, $x_i$, $\mathcal{S}$} \;
\lIf{$t \neq y_i$}{remove chosen $\left(x_i, y_i\right)$ from $\mathcal{S}$}
}
$\mathcal{R}_0 \leftarrow \emptyset$ \;
add random $\left(x_i, y_i\right)$ to $\mathcal{S}$ \;

\Repeat{$\mathcal{R}_0 = \mathcal{R}_1$}{
$\mathcal{R}_1 \leftarrow \mathcal{R}_0$ \;
\For{$\left(x_i, y_i\right) \in \mathcal{S}$}{
$s \leftarrow $\FkNN{$k=1$, $x_i$, $\mathcal{R}_1$} \;
\lIf{$s \neq y_i$}{add $\left(x_i, y_i\right)$ to $\mathcal{R}_1$}
}
}
\Return{$\mathcal{R}_1$}
\caption{The condensed nearest neighbours algorithm. This algorithm relies on an implementation of the $k$-NN, represented here by the $\mathtt{kNN}$ function that takes as input the number of neighbours $k$, the data-points to be classified $x_i$ and the set in which it should search for the neighbours $\mathcal{S}$.}
\label{alg:cnn}
\end{algorithm}
\end{center}

\FloatBarrier