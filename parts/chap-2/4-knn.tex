\section{Nearest neighbours}
A classification algorithm that is often used in intrusion detection systems is the $k$-nearest neighbour ($k$-NN). This is a simple and effective method based on the distance of the elements in the feature space. Intuitively, when we want to characterize new data elements, we compare it to existent data elements and try to assign to it the characteristics --- in this case, the intrusion target --- of the similar ones. In other words, the test set elements are classified as similar training elements. Though, how do we count elements to be similar, how do we measure similarity? In the $k$-NN algorithm, the elements are considered as data-points in the feature space and the similarity is measured as the euclidean distance between them. The most common target of the similar elements is then assigned to the test element.

More concretely, let's first consider a binary classifier of observations and targets $\left( x_1, y_1 \right), \ldots , \left( x_n, y_n \right)$ where observations $x_i \in \mathbb{R}^d$ and targets $y_i \in \left\{0,1\right\}$. To classify a new observation $\left( x_i, y_i \right)$, we just have to compare it to the $k$ other nearest observations and assign to the test observation the dominant target among its $k$ neighbours. In the classical $k$-NN algorithm, the neighbours are defined by the Euclidean distance. However, because of the monotony of the square function, one may use the square of the distances instead, avoiding to compute an extra square root, which is very useful in frameworks where operations are expensive like MPCs:
\begin{equation}
    \mathrm{d}^2\left( x_i, x_j \right) = \norm{x_i-x_j}^2 = \sum_{k=1}^d \left( x_{ik}, x_{jk} \right)^2.
\end{equation}

Unlike binary classification algorithms like SVMs, $k$-NN algorithms are suited for multi-class problems without requiring any construction. However, they have also been used for binary classification in a construction \cite{Aburomman2016ASystem}. 

\subsection{Condensed Nearest Neighbours Data Reduction}
The problem of the nearest neighbours algorithm is the time it takes to compute for large data-bases. First, the computation of the distances increases linearly with the number of training points for each test points. Secondly, finding the nearest neighbours requires at least each distance to be touched once, a linear complexity being thus here a lower bound. As we will see, one of the main trade-offs of MPC is the drastic increase in execution time. Therefore, the reduction of data-points becomes a necessity for the use of MPC-based $k$-NN.

The \emph{condensed nearest neighbours data reduction} (CNN) algorithm aims at finding the only relevant data-points in the whole training set. The unclassified data-points then only need to be compared with the prototype points and not the whole data-base anymore. It is based on the idea that misclassified data-points lie close to the decision boundary, and one can just keep the data-points near the decision boundary and discard the others. The goal of the algorithm is to classify all training data-points into three categories:
\begin{itemize}
    \item \textbf{Outliers:} points which would not be recognized as the correct type if added to the data-base later. In other words, these data-points are the ones making the model perform worse with them than without. They increase the complexity of the decision boundary and increase significantly the number of kept data-points and thus the relevance of the algorithm as it tries to keep a minimum of points defining the decision boundary. These points are to be discarded.
    \item \textbf{Prototypes:} the minimum set of points required in the training set for all the other non-outlier points to be correctly recognized. These data-points thus contain almost all of the relevant data. These points define the decision boundary and are to be kept.
    \item \textbf{Absorbed points:} points which are not outliers, and would be correctly recognized based on the sole prototype points. This could also be characterized as the redundant data. These points are far from the decision boundary and are to be discarded.
\end{itemize}

The outliers are first found by testing all existing points against the rest of the data-base with the chosen number $k$ of neighbours. If the point isn't recognized as the correct class, it is considered as an outlier, otherwise not. After defining all the outliers, one can proceed into the classification the remaining data-points as prototypes or absorbed points. This is done as following. Each point is once again tested against the data-set without the outliers, but with one sole neighbour, i.e. $k=1$. If it is well classified, it is considered as an absorbed point and can be left out of the final data-set. Otherwise, it is considered as a prototype ans is considered relevant to the classification or not redundant. These data-points are the sole ones making it to the final data-set. All points are tested until no prototype anymore makes it into the final data-set. The CNN data-reduction is given in algorithm~\ref{alg:cnn}.

In the condensed nearest neighbours algorithm, all new data-points are tested against the reduced data-base. Another big advantage of this algorithm is that the algorithm now always has to be used with $k=1$ because of the way we defined our prototypes and the final algorithm is thus faster. However, the classification with CNN will most of the time lead to a slightly different classification than with the classical $k$-NN against the whole data-set.

\begin{center}
\begin{algorithm}[H]
 \KwData{Set $\mathcal{S}$ of $n$ feature points $\left\{x_i\right\}_{i=1 \ldots n}$ and corresponding targets $\left\{y_i\right\}_{i=1 \ldots n}$, number of neighbours $k$ for outlier detection}
 \KwResult{Reduced set $\mathcal{R}$ of $m<n$ feature points $\left\{x_i\right\}_{i=1 \ldots m}$ and corresponding targets $\left\{y_i\right\}_{i=1 \ldots m}$}
\DontPrintSemicolon
\SetKwFunction{FkNN}{kNN}

\ForEach{$\left(x_i, y_i\right) \in \mathcal{S}$}{
$t \leftarrow $\FkNN{$k$, $x_i$, $\mathcal{S}$} \;
\lIf{$t \neq y_i$}{remove chosen $\left(x_i, y_i\right)$ from $\mathcal{S}$}
}
$\mathcal{R}_0 \leftarrow \emptyset$ \;
add random $\left(x_i, y_i\right)$ to $\mathcal{S}$ \;

\Repeat{$\mathcal{R}_0 = \mathcal{R}_1$}{
$\mathcal{R}_1 \leftarrow \mathcal{R}_0$ \;
\For{$\left(x_i, y_i\right) \in \mathcal{S}$}{
$s \leftarrow $\FkNN{$k=1$, $x_i$, $\mathcal{R}_1$} \;
\lIf{$s \neq y_i$}{add $\left(x_i, y_i\right)$ to $\mathcal{R}_1$}
}
}
\Return{$\mathcal{R}_1$}
\caption[The condensed nearest neighbors algorithm.]{The condensed nearest neighbours algorithm. This algorithm relies on an implementation of the $k$-NN, represented here by the $\mathtt{kNN}$ function that takes as input the number of neighbours $k$, the data-points to be classified $x_i$ and the set in which it should search for the neighbours $\mathcal{S}$.}
\label{alg:cnn}
\end{algorithm}
\end{center}