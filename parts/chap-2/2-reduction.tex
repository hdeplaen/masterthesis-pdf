\section{Data reduction}
As we will see in chapter~\ref{cha:3}, making an algorithm privacy-friendly is proportionally more costly: every operation takes much more time and are underlyingly based on a whole bunch of other sub-operations. Algorithms that may run with an acceptable speed in normal circumstances may become too slow to be used in practice in their privacy-reserving form. The need for data reduction thus becomes crucial. The algorithms presented in this section aim at reducing the data as much as possible, to suppress redundancy or information that is not relevant to the performance of the classification algorithms. Every information that is not crucial and kept will participate to the algorithm and unnecessarily slow it down.

We investigate here two main classes of data reduction. Instance-set size reduction aims at reducing the number of points used in the training of the algorithm and often find themselves in the final information used for the evaluation of this algorithm against a new data-point (a query). Feature-size reduction aims at reducing the size of the feature vector used as an input in the algorithm. This can also drastically reduce the speed of the algorithm. As an example, most of the algorithms that we present work by calculating the euclidean distance between two feature vectors at their core. The evaluation of an euclidean distance has a complexity depending linearly on the size of the vector. Reducing the feature vector size by 2 could reduce the algorithm speed by the same factor if the computation of the distance takes place in every iteration for example.

\subsection{Instance-set size reduction}
Instance-based learning algorithms are often faced with the problem of deciding which instances to store for use during generalization. A general principle of machine-learning is that the variance of its prediction always theoretically decreases if the training data-set increases --- under a few assumptions. In other words, this means that the more points are used to train the algorithm, the better it performs. This is not a very surprising property as machine-learning algorithms are in essence just complex statistical models, for which the notion of increasing the sample size to reduce the error is very common. Reducing the instance-set must be done in a clever way to be able to keep a similar --- or at least not significantly worse --- performance as with the whole data-set.  

In practice, data can be classified into two kind of classes, attack and no attack, or normal. In a practical intrusion detection system, most of the traffic is normal and only a small proportion is attacking. We thus want to reduce the size of the normal data-set to make it of a similar size to the attack classes. Furthermore, the normal instances are very diversified and one may not just reduce the set randomly at the risk of missing relevant instances. One must thus find a more intelligent heuristic to decide which normal instances to keep.

How can we decide which instances to keep? The idea is to keep only the ones near the decision boundary. If we consider all the data-points in a hyper-space of the dimension of the feature vector, the decision boundary is the set of hyper-surfaces separating the different classes. Indeed, if we classify all the points contained in the hyper-space --- more accurately called varieties ---, each one will be attributed to a certain class and points of the same class will then form regions in this hyper-space. The decision boundary is the boundaries of these regions, the varieties separating these regions. The idea behind the selection of the points near the decision boundaries is that points far from them have a small impact on them. A data-point that is very different from another one will have no impact on its classification. In other words, points have only a local impact in the hyper-space. This idea is at the core of a lot of machine-learning algorithms.

\subsubsection{$k$-means clustering}
The $k$-means clustering algorithm is a semi-supervised instance set reduction algorithm. This a specific case of clustering methods, that aim to divide the original data-set into smaller $k$ smaller clusters that should share some common properties. The challenge here is to find the most homogeneous possible clusters, which instances are similar enough --- referred to as the minimization of \emph{intra-class inertia} ---, but not too many so that the clusters are well differentiated -- also referred to as the maximisation of \emph{inter-class inertia}. After this being achieved, one can thus reduce each cluster far from the decision boundary to an archetype instance of this cluster and keep the clusters near the decision boundary. The algorithm is semi-supervised as the clustering in itself is only based on the features set, but is only applied on the data-points that are classified as normal, which of course depends on the target space.

To determine the different clusters, the $k$-means algorithm tries to minimize the distance between the instances in each cluster $\mathcal{S}_i$. The number of clusters is fixed to $k$, which an hyper-parameter of the model set by the user. Computing the distance between each of the points would require a quadratic at each evaluation of cluster. To keep the complexity linear, the distances are not computed between each of the instances, but between each of the instances and the mean of these instances $\mu_i = \left( \sum_{x_j \in \mathcal{S}_i} x_j\right)/\vert \mathcal{S}_i \vert$ where $\vert \mathcal{S}_i \vert$ is the number of instances in the cluster
\begin{equation}
    \underset{\mathcal{S}}{\mathrm{arg}\,\mathrm{min}} \sum_{i=1}^k \sum_{x_i \in \mathcal{S}_i} \norm{x_j-\mu_i}^2,
\end{equation}
with the objective function $\sum_{i=1}^k \sum_{x_i \in \mathcal{S}_i} \norm{x_j-\mu_i}^2$ being called the \emph{within-cluster distance}. 

Defining the optimal clusters would at least be exponentially complex in function of the number of total instances as each cluster combination would have to be tested. Therefore, the $k$-means algorithm starts with assigning the starting clusters to initial data-points, chosen at random in our case. It then goes through the whole data-set and assigns each point to the cluster with the nearest mean $\mu_i$. The means are then updated and the process starts again. This is given at algorithm~\ref{alg:k-means}. Although this algorithm doesn't guarantee any optimality nor polynomial computational time, it is considered very effective in the practice \cite{Arthur2006Worst-caseMethod}.

\begin{center}
\begin{algorithm}[H]
 \KwData{Feature data-set $\mathcal{F} = \left\{x_i\right\}_{i=1 \ldots n}$ and number of cluster sets $k$}
 \KwResult{$k$ clusters $\mathcal{S}_i = \left\{x_i\right\}_{i=1 \ldots m_i}$}
\DontPrintSemicolon

\lForEach{i = 1\ldots k}{
$\mu_i \leftarrow$ random $x_j$
}
\Repeat{$\mathrm{convergence}$}{
\ForEach{$x_j \in \mathcal{F}$}{
\lForEach{$i = 1\ldots k$}{$d_{i} \leftarrow \norm{x_j-\mu_i}$}
$l \leftarrow \mathrm{arg}\,\mathrm{min} d_i$ \;
assign $x_j$ to $\mathcal{S}_l$ \;
}
\ForEach{$i = 1\ldots k$}{
$\mu_i \leftarrow \mu_i = \frac{1}{\vert \mathcal{S}_i \vert}\left( \sum_{x_j \in \mathcal{S}_i} x_j\right)$
}
}
\Return{$\mathcal{S}_{1 \ldots k}$}
\caption[The $k$-means algorithm.]{The $k$-means algorithm. The convergence criterion typically is no more evolution in the means or the composition of the clusters, which is almost always equivalent in the practice.}
\label{alg:k-means}
\end{algorithm}
\end{center}

Once the clusters are determined, we can remove the normal data instances that are the farthest from the decision boundary and replace the whole cluster by its mean $\mu_i$. In this way, we reduce the number of points far from the decision boundary and keep all the ones near the decision boundary. The number of set to be reduced $p<k$ is also an hyper-parameter set by the user. A way to determine which one are the farthest is just to compute the distance between the mean of each cluster and the mean of the attack instances defined in the same way.

\subsection{Feature size reduction}
In the analogy of the hyper-space developed here-above, the feature-size was the dimension of the hyper-space. Reducing the feature size means working in a smaller hyper-space. This also means the ability of having less entangled boundaries to separate our classes. This suppresses complexity of the model, which is generally a good thing.

The main task in statistical learning is to avoid any \emph{overfitting}: any useless addition of complexity may allow the regression to capture the variance of the specific data-set on which it is trained on top of the underlying relation is it supposed to capture. The regression would then reproduce the training set itself instead of generalizing it. I therefore would like to cite John von Neumann
\begin{displayquote}
\emph{With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.}
\end{displayquote}

In other words, one parameter is sufficient to add a lot of complexity to the model and increase a lot the risk of overfitting. The general rule of thumb is thus to always reduce as much as possible the complexity of the model and always stick to the lowest one able to reproduce the general scheme of the data. This principle is also known as \emph{Occam's razor}.

However, we must reduce the feature-size in a clever way, using a mapping that takes into account the possible entanglement of our boundaries in the higher dimension hyper-space.

\subsubsection{Principal components analysis}
Let's consider a feature vector $x = \left[x^1,\ldots,x^{n_f} \right]^T$ where $n_f$ is the number of features (the exponent notation is used to avoid confusion with the indices which represent the different feature vector instances). A linear Principal Components Analysis (PCA) dimensionality reduction consists in a spectral analysis of the covariance matrix defined by
\begin{equation}
    \hat{\Sigma}_{ij} = \mathbb{E}\left[\left(x^i - \mathbb{E}[x^i]\right)\left(x^j - \mathbb{E}[x^j]\right)\right],
\end{equation} 
where $\mathbb{E}(x)$ represents statistical expected value of $x$. The covariance matrix is in fact a generalization of the variance to multiple dimensions and its elements actually measure the (linear) correlation between two variables and its diagonal the variances. This covariance matrix can diagonalized with nonnegative eigenvalues $\lambda_i$
\begin{equation}
\hat{\Sigma}x = \lambda_ix
\end{equation}
The $M$ greatest eigenvalues are then kept and the data are transformed accordingly:
\begin{equation}
z_i = x_i^Tx.
\end{equation}

Here is an attempt of an intuitive understanding of the process. Each input variable of the original input vector is independent, but some have strong underlying relations between them, measured by their covariance matrix. By diagonalizing it, there result $n_f$ new variables without any linear underlying relation. In a certain way, the eigenvectors $x_j$ represent these relations and their respective eigenvalues $\lambda_j$ their contribution, thus their importance. We can then afford to drop the non-important ones.

In our case, each feature is first being normalized, i.e. zero mean and unit variance. In other words, the covariance matrix has a unit diagonal and with a feature dimension of $n_f$, we have $tr(\hat{\mathbf{\Sigma}})=n_f$ which corresponds in a certain way to the total variance of the whole feature space. Quite logically, the latter is invariant to any linear transformation as the trace is an invariant to any linear transformation. The main idea behind linear PCA analysis reduction is thus to keep the richness of the data to a maximum, thus its total variance, and reducing its dimension a maximum by dropping its smallest contributors. A good measure for the relative importance of each eigenvector, or underlying linear relation, is its fraction or variance contribution given by
\begin{equation}
f_j = \frac{\lambda_j}{\Sigma_{\forall j}\lambda_j} = \frac{\lambda_j}{\tr\left(\hat{\mathbf{\Sigma}}\right)}.
\end{equation}

One can then choose the input values it decides to keep in descending order of the respective fractions $f_j$.

\subsubsection{Chi-square feature selection}
Up to now, the idea was to reduce the feature size by constructing new ones as transformations (or mappings) of the original feature vectors. However, we can also choose to select a number of features and just not care about the other ones by simply discarding them. Another interesting approach would be to be able to reduce the feature size in a supervised way, taking the machine learning algorithm into account. These ideas can be tackled with $\chi^2$ feature reduction.

This feature selection method is based on the $\chi^2$ statistical hypothesis test that measures the dependence of two categorical variables. Two variables are independent if for all categories $A$ and $B$
\begin{equation}
    P(A, B) = P(A) \times P(B).
\end{equation}

This value is observed to be $N_{AB}$, the frequency of respective categories for each variable $A$ and $B$ happening together. This is value is expected to be
\begin{equation}
    E_{AB} = N_{A\cdot} \times N_{\cdot B},
\end{equation}
with $N_{A\cdot} = \sum_{\forall B} N_{AB}$ and respectively $N_{\cdot B} = \sum_{\forall A} N_{AB}$. Pearson showed in 1900 that the distance between these observed and the expected values follows a $\chi^2$ distribution of degrees of freedom $DF = (c_A-1)(c_B-1)$ where $c_A$ and $c_B$ are the number of possible categories for $A$ and $B$
\begin{equation}
    \chi^2_{DF} \sim T = \sum_{\forall A,B} \frac{\left(N_{AB}-E_{AB}\right)^2}{E_{AB}}.
\end{equation}

In our case, the problem is that almost all of the input features are non-categorical and all of them are after pre-processing. A solution could be binning\footnote{This is a step of transforming the input data again into categorical data putting into different bins. This is a just a discretization into intervals of the distribution each feature.}. However, this would not allow us to do this while taking the classification algorithm into account and thus creating a selection tailored for the specific classification algorithm. This is why we define here a $\chi^2$ metric to compare the output classification with the expected one. We will only work with two categories, true and false. This gives us the following metric
\begin{eqnarray}
    \chi^2\mathrm{-metric} &=& \frac{\left(t_p - (t_p+f_p)N_p \right)^2}{(t_p+f_p)N_p} + \frac{\left(tn - (f_n+t_n)N_p \right)^2}{(f_n+t_n)N_p} \\
    &+& \frac{\left(f_p - (t_p+f_p)N_n \right)^2}{ (t_p+f_p)N_n} + \frac{\left(t_n-(f_n+t_n)N_p \right)^2}{(f_n+t_n)N_p},
\end{eqnarray}
where $t_p$ is the number of true positives, $t_n$ the number of true negatives, $f_p$ the number of false positives and $f_n$ the number of false negatives.

Based on this metric, we can now now select the relevant features in an iterative manner. Each feature is temporary selected as being the only one in the feature vector and the resulting model is trained and tested through this metric. If the metric is smaller than a certain threshold, it is definitely deleted and otherwise restored. The process can then start again with a new feature until all have been tested. In this way, the algorithm is also independent of the order in which the features are tested. This allows to find an optimal set of features.

It has been observed that $\chi^2$ feature selection performs very well against multi-class data-sets compared to other feature selection methods \cite{Yang:1997:CSF:645526.657137}. It has also successfully been applied to some cases of intrusion detection systems and seem to deliver better results than PCA \cite{SumaiyaThaseen2017IntrusionSVM}.